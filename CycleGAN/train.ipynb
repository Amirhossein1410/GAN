{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1D8lDVmGf7YGd0sN9zTwgAl-kJhPVrOzu","authorship_tag":"ABX9TyOxrwjdlhwInZq22nAbGSc9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"whAbPVc0mERz","executionInfo":{"status":"ok","timestamp":1703521934080,"user_tz":-210,"elapsed":1036,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/Colab/GAN/CycleGAN')"]},{"cell_type":"code","source":["import torch\n","from dataset import HorseZebraDataset\n","import sys\n","from utils import save_checkpoint , load_checkpoint\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torch.nn as nn\n","import torch.optim as optim\n","import Config\n","from tqdm import tqdm\n","from torchvision.utils import save_image\n","from discriminator_model import Discriminator\n","from generator_model import Generator\n","\n","def train_fn(disc_H , disc_Z , gen_Z , gen_H , loader , opt_disc , opt_gen , l1 , mse , d_scaler , g_scaler):\n","    H_reals = 0\n","    H_fakes = 0\n","    loop = tqdm(loader , leave=True)\n","\n","    for idx, (zebra, horse) in enumerate(loop):\n","        zebra = zebra.to(Config.DEVICE)\n","        horse = horse.to(Config.DEVICE)\n","\n","        #Train Discriminator H and Z\n","        with torch.cuda.amp.autocast():\n","            fake_horse = gen_H(zebra)\n","            D_H_real = disc_H(horse)\n","            D_H_fake = disc_H(fake_horse.detach())\n","            H_reals += D_H_real.mean().item()\n","            H_fakes += D_H_fake.mean().item()\n","            D_H_real_loss = mse(D_H_real , torch.ones_like(D_H_real))\n","            D_H_fake_loss = mse(D_H_fake , torch.zeros_like(D_H_fake))\n","            D_H_loss = D_H_real_loss + D_H_fake_loss\n","\n","            fake_zebra = gen_Z(horse)\n","            D_Z_real = disc_Z(zebra)\n","            D_Z_fake = disc_Z(fake_zebra.detach())\n","            D_Z_real_loss = mse(D_Z_real , torch.ones_like(D_Z_real))\n","            D_Z_fake_loss = mse(D_Z_fake , torch.zeros_like(D_Z_fake))\n","            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n","\n","            #put it together\n","            D_loss = (D_H_loss +D_Z_loss)/2\n","\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train generator H and Z\n","        with torch.cuda.amp.autocast():\n","            D_H_fake = disc_H(fake_horse)\n","            D_Z_fake = disc_Z(fake_zebra)\n","            loss_G_H = mse(D_H_fake , torch.ones_like(D_H_fake))\n","            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","            # Cycle loss\n","            cycle_zebra = gen_Z(fake_horse)\n","            cycle_horse = gen_H(fake_zebra)\n","            cycle_zebra_loss = l1(zebra , cycle_zebra)\n","            cycle_horse_loss = l1(horse , cycle_horse)\n","\n","            #identity loss\n","            identity_zebra = gen_Z(zebra)\n","            identity_horse = gen_H(horse)\n","            identity_zebra_loss = l1(zebra , identity_zebra)\n","            identity_horse_loss = l1(horse , identity_horse)\n","\n","            #add all together\n","            G_loss = (\n","                loss_G_Z\n","                + loss_G_H\n","                + cycle_zebra_loss * Config.LAMBDA_CYCLE\n","                + cycle_horse_loss * Config.LAMBDA_CYCLE\n","                + identity_horse_loss * Config.LAMBDA_IDENTITY\n","                + identity_zebra_loss * Config.LAMBDA_IDENTITY\n","            )\n","\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","        if idx % 200 == 0:\n","            save_image(fake_horse * 0.5 + 0.5 , f\"saved_images/horse_{idx}.png\")\n","            save_image(fake_zebra * 0.5 + 0.5 , f\"saved_images/zebra_{idx}.png\")\n","\n","        loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))\n","\n","\n","def main():\n","    disc_H = Discriminator(in_channels=3).to(Config.DEVICE)\n","    disc_Z = Discriminator(in_channels=3).to(Config.DEVICE)\n","    gen_Z = Generator(img_channels=3, num_residuals=9).to(Config.DEVICE)\n","    gen_H = Generator(img_channels=3, num_residuals=9).to(Config.DEVICE)\n","    opt_disc = optim.Adam(\n","        list(disc_H.parameters()) + list(disc_Z.parameters()),\n","        lr=Config.LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    opt_gen = optim.Adam(\n","        list(gen_Z.parameters()) + list(gen_H.parameters()),\n","        lr=Config.LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    L1 = nn.L1Loss()\n","    mse = nn.MSELoss()\n","\n","    if Config.LOAD_MODEL:\n","        load_checkpoint(\n","            Config.CHECKPOINT_GEN_H,\n","            gen_H,\n","            opt_gen,\n","            Config.LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            Config.CHECKPOINT_GEN_Z,\n","            gen_Z,\n","            opt_gen,\n","            Config.LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            Config.CHECKPOINT_CRITIC_H,\n","            disc_H,\n","            opt_disc,\n","            Config.LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            Config.CHECKPOINT_CRITIC_Z,\n","            disc_Z,\n","            opt_disc,\n","            Config.LEARNING_RATE,\n","        )\n","\n","    dataset = HorseZebraDataset(\n","        root_horse=Config.TRAIN_DIR + \"/trainA\",\n","        root_zebra=Config.TRAIN_DIR + \"/trainB\",\n","        transform=Config.transforms,\n","    )\n","\n","    val_dataset = HorseZebraDataset(\n","        root_horse=Config.VAL_DIR + '/testA',\n","        root_zebra=Config.VAL_DIR + '/testB',\n","        transform=Config.transforms,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=1,\n","        shuffle=False,\n","        pin_memory=True,\n","    )\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=Config.BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=Config.NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(Config.NUM_EPOCHS):\n","        train_fn(\n","            disc_H,\n","            disc_Z,\n","            gen_Z,\n","            gen_H,\n","            loader,\n","            opt_disc,\n","            opt_gen,\n","            L1,\n","            mse,\n","            d_scaler,\n","            g_scaler,\n","        )\n","\n","        if Config.SAVE_MODEL:\n","            save_checkpoint(gen_H, opt_gen, filename=Config.CHECKPOINT_GEN_H)\n","            save_checkpoint(gen_Z, opt_gen, filename=Config.CHECKPOINT_GEN_Z)\n","            save_checkpoint(disc_H, opt_disc, filename=Config.CHECKPOINT_CRITIC_H)\n","            save_checkpoint(disc_Z, opt_disc, filename=Config.CHECKPOINT_CRITIC_Z)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPth1qTZmJYQ","outputId":"6e4069c9-bf19-41df-a103-5043e6c49c2b","executionInfo":{"status":"ok","timestamp":1703522343211,"user_tz":-210,"elapsed":376182,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["=> Loading checkpoint\n","=> Loading checkpoint\n","=> Loading checkpoint\n","=> Loading checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","100%|██████████| 1334/1334 [05:23<00:00,  4.12it/s, H_fake=0.426, H_real=0.571]\n"]},{"output_type":"stream","name":"stdout","text":["=> Saving checkpoint\n","=> Saving checkpoint\n","=> Saving checkpoint\n","=> Saving checkpoint\n"]}]},{"cell_type":"code","source":["disc_H = Discriminator(in_channels=3).to(Config.DEVICE)\n","disc_Z = Discriminator(in_channels=3).to(Config.DEVICE)\n","gen_Z = Generator(img_channels=3, num_residuals=9).to(Config.DEVICE)\n","gen_H = Generator(img_channels=3, num_residuals=9).to(Config.DEVICE)\n","opt_disc = optim.Adam(\n","    list(disc_H.parameters()) + list(disc_Z.parameters()),\n","    lr=Config.LEARNING_RATE,\n","    betas=(0.5, 0.999),\n",")\n","\n","opt_gen = optim.Adam(\n","    list(gen_Z.parameters()) + list(gen_H.parameters()),\n","    lr=Config.LEARNING_RATE,\n","    betas=(0.5, 0.999),\n",")\n","\n","L1 = nn.L1Loss()\n","mse = nn.MSELoss()\n","\n","load_checkpoint(\n","  Config.CHECKPOINT_GEN_H,\n","  gen_H,\n","  opt_gen,\n","  Config.LEARNING_RATE,\n",")\n","load_checkpoint(\n","  Config.CHECKPOINT_GEN_Z,\n","  gen_Z,\n","  opt_gen,\n","  Config.LEARNING_RATE,\n",")\n","load_checkpoint(\n","  Config.CHECKPOINT_CRITIC_H,\n","  disc_H,\n","  opt_disc,\n","  Config.LEARNING_RATE,\n",")\n","load_checkpoint(\n","  Config.CHECKPOINT_CRITIC_Z,\n","  disc_Z,\n","  opt_disc,\n","  Config.LEARNING_RATE,\n",")\n","\n","\n","\n","\n","val_dataset = HorseZebraDataset(\n","    root_horse=Config.VAL_DIR + '/testA',\n","    root_zebra=Config.VAL_DIR + '/testB',\n","    transform=Config.transforms,\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=1,\n","    shuffle=False,\n","    pin_memory=True,\n",")\n","\n","\n","\n","g_scaler = torch.cuda.amp.GradScaler()\n","d_scaler = torch.cuda.amp.GradScaler()"],"metadata":{"id":"P7v2w_-9mPC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703330003559,"user_tz":-210,"elapsed":1815,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}},"outputId":"fa76a317-dd0a-4ac6-f9e3-fec9e4d6cd49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=> Loading checkpoint\n","=> Loading checkpoint\n","=> Loading checkpoint\n","=> Loading checkpoint\n"]}]},{"cell_type":"code","source":["def testing(disc_H , disc_Z , gen_Z , gen_H , loader , opt_disc , opt_gen , l1 , mse , d_scaler , g_scaler):\n","    H_reals = 0\n","    H_fakes = 0\n","    loop = tqdm(loader , leave=True)\n","\n","    for idx, (zebra, horse) in enumerate(loop):\n","        zebra = zebra.to(Config.DEVICE)\n","        horse = horse.to(Config.DEVICE)\n","\n","        #Train Discriminator H and Z\n","        with torch.cuda.amp.autocast():\n","            fake_horse = gen_H(zebra)\n","            D_H_real = disc_H(horse)\n","            D_H_fake = disc_H(fake_horse.detach())\n","            H_reals += D_H_real.mean().item()\n","            H_fakes += D_H_fake.mean().item()\n","            D_H_real_loss = mse(D_H_real , torch.ones_like(D_H_real))\n","            D_H_fake_loss = mse(D_H_fake , torch.zeros_like(D_H_fake))\n","            D_H_loss = D_H_real_loss + D_H_fake_loss\n","\n","            fake_zebra = gen_Z(horse)\n","            D_Z_real = disc_Z(zebra)\n","            D_Z_fake = disc_Z(fake_zebra.detach())\n","            D_Z_real_loss = mse(D_Z_real , torch.ones_like(D_Z_real))\n","            D_Z_fake_loss = mse(D_Z_fake , torch.zeros_like(D_Z_fake))\n","            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n","\n","            #put it together\n","            D_loss = (D_H_loss +D_Z_loss)/2\n","\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train generator H and Z\n","        with torch.cuda.amp.autocast():\n","            D_H_fake = disc_H(fake_horse)\n","            D_Z_fake = disc_Z(fake_zebra)\n","            loss_G_H = mse(D_H_fake , torch.ones_like(D_H_fake))\n","            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","            # Cycle loss\n","            cycle_zebra = gen_Z(fake_horse)\n","            cycle_horse = gen_H(fake_zebra)\n","            cycle_zebra_loss = l1(zebra , cycle_zebra)\n","            cycle_horse_loss = l1(horse , cycle_horse)\n","\n","            #identity loss\n","            identity_zebra = gen_Z(zebra)\n","            identity_horse = gen_H(horse)\n","            identity_zebra_loss = l1(zebra , identity_zebra)\n","            identity_horse_loss = l1(horse , identity_horse)\n","\n","            #add all together\n","            G_loss = (\n","                loss_G_Z\n","                + loss_G_H\n","                + cycle_zebra_loss * Config.LAMBDA_CYCLE\n","                + cycle_horse_loss * Config.LAMBDA_CYCLE\n","                + identity_horse_loss * Config.LAMBDA_IDENTITY\n","                + identity_zebra_loss * Config.LAMBDA_IDENTITY\n","            )\n","\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","\n","        save_image(fake_horse * 0.5 + 0.5 , f\"results/horse_{idx}.png\")\n","        save_image(fake_zebra * 0.5 + 0.5 , f\"results/zebra_{idx}.png\")\n","\n","        loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))"],"metadata":{"id":"J4RVNjNNS0f6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(disc_H,\n","      disc_Z,\n","      gen_Z,\n","      gen_H,\n","      val_loader,\n","      opt_disc,\n","      opt_gen,\n","      L1,\n","      mse,\n","      d_scaler,\n","      g_scaler,\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FyrdYOgWwVI","executionInfo":{"status":"ok","timestamp":1703330191147,"user_tz":-210,"elapsed":60953,"user":{"displayName":"Amir Hossein Hosseini","userId":"00054851132641383807"}},"outputId":"e3b1ac8c-f6f0-45dc-beec-eb1ca422a323"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 140/140 [01:00<00:00,  2.30it/s, H_fake=0.25, H_real=0.742]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"P4NtM-4uYJsl"},"execution_count":null,"outputs":[]}]}